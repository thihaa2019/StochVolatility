---
title: "quiz-3-mh"
author: "Nathan Ng"
date: "8/19/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



MCMC (Markov Chain Monte Carlo) simulation produces a _chain_ of $N$ __dependent__ $\pi$ values, $\left\lbrace \pi^{(1)}, \pi^{(2)}, \ldots, \pi^{(N)} \right\rbrace$, which are __not__ drawn from the posterior pdf $f(\pi|x)$.

__Metropolis-Hastings algorithm__

Step 1: Propose a random location for the parameter.
Step 2: Decide whether to go to the proposed location or to stay at the current location


## Approximating Posterior

$\pi \sim  \text{Beta}(4,1)$  
$X = 3$

We know posterior will be Beta(4,1)

```{r message=FALSE}
library(tidyverse)
```

```{r}
currentDB <- 0.7
set.seed(4)
```

MH algorithm makes a proposal

```{r}
proposalDB <- 0.4
proposalDB
```


```{r}
bayesrules::plot_normal(currentDB, 0.3)
```


```{r}
proposal_plausDB <- dbeta(proposalDB, 4, 1) * dbinom(3, 5,proposalDB)

current_plausDB <- dbeta(currentDB , 4, 1) * dbinom(3, 5,currentDB)

alphaDB <- min(1, proposal_plausDB/current_plausDB)
alphaDB
```

Should I stay or should I go?

```{r}
next_stopDB <- sample(c(proposalDB, currentDB), 
                    size = 1, 
                    prob = c(alphaDB, 1-alphaDB))
next_stopDB
```



```{r}
one_mh_iterationDB <- function(sigma, currentDB){
  
  proposalDB <- rnorm(1,mean=currentDB,sigma)
  
 if (proposalDB < 0) {
alphaDB <- 0

} else {
    
 
    
   proposal_plausDB <- dbeta(proposalDB, 4, 1) * dbinom(3, 5,proposalDB)

   current_plausDB <- dbeta(currentDB , 4, 1) * dbinom(3, 5,currentDB)
    
    alphaDB <- min(1, proposal_plausDB/current_plausDB)
} 
  
  next_stopDB <- sample(c(proposalDB, currentDB), 
                    size = 1, 
                    prob = c(alphaDB, 1-alphaDB))

  return(data.frame(proposalDB, alphaDB, next_stopDB))
  
} #end of one_mh_iteration function
```

Using the `one_mh_iteration` function

```{r}
set.seed(4)

one_mh_iterationDB(sigma = 0.3, currentDB = .7)
```
  
The code above implements one iteration of the Metropolis-Hastings algorithm but we would like to have multiple iterations.


```{r}
mh_tourDB <- function(N, sigma){
  
  # Start the chain at location 1
  currentDB <- 0.7
  
  # Initialize the simulation
  
  piDB <- rep(0, N)
  
  # Simulate N Markov chain stops
  
  for(i in 1:N){
    
    #Simulate one iteration
    sim <- one_mh_iterationDB(sigma = sigma, 
                            currentDB = currentDB)
    
    #Record next location
    piDB[i] <- sim$next_stop
    #print(sim$next_stop)
    #print(lambda[i])
    #Reset the current location
    currentDB <- sim$next_stop
    #print(current)

  }
  #Return the chain locations
  
  return(data.frame(iteration = c(1:N), piDB))
}
```

Using the Metropolis Hastings Algorithm to approximate the posterior. Recall:

$\pi \sim  \text{Beta}(4,1)$  
$X = 3$  

We know posterior will be Beta(7,3)

```{r}
set.seed(4)
mh_simulation_1DB <- mh_tourDB(N = 5000, sigma =0.3)

mh_simulation_1DB %>% 
  ggplot(aes(x = iteration, y =piDB )) +
  geom_line()

mh_simulation_1DB %>% 
  ggplot(aes(x = piDB)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dbeta, args = list(7,3))

```

##Q1 lambda = 13, n=5000
```{r}
set.seed(4)
mh_simulation_1 <- mh_tour(N = 5000, sigma =0.3)

mh_simulation_1 %>% 
  ggplot(aes(x = iteration, y = lambda)) +
  geom_line()

mh_simulation_1 %>% 
  ggplot(aes(x = lambda)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dgamma, args = list(1,2))

```


##Q2
## Approximating Posterior, sigma=3

$\lambda \sim  \text{Gamma}(1,1)$  
$X = 0$

We know posterior will be Gamma(1,2)

```{r message=FALSE}
library(tidyverse)
```

```{r}
current <- 13
set.seed(4)
```

MH algorithm makes a proposal

```{r}
proposal <- rnorm(1, mean = current, sd = 3)
proposal
```


```{r}
bayesrules::plot_normal(current, 3)
```


```{r}
proposal_plaus <- dgamma(proposal, 1, 1) * dpois(0, proposal)

current_plaus <- dgamma(current, 1, 1) * dpois(0, current)

alpha <- min(1, proposal_plaus/current_plaus)
alpha
```

Should I stay or should I go?

```{r}
next_stop <- sample(c(proposal, current), 
                    size = 1, 
                    prob = c(alpha, 1-alpha))
next_stop
```



```{r}
one_mh_iteration <- function(sigma, current){
  
  proposal <- rnorm(1, mean = current, sd = sigma)
  
  if (proposal < 0) {
    alpha <- 0
    
  } else {
    
    proposal_plaus <- dgamma(proposal, 1, 1) * dpois(0, proposal)
    
    current_plaus <- dgamma(current, 1, 1) * dpois(0, current)
    
    alpha <- min(1, proposal_plaus/current_plaus)
  }   
  
  next_stop <- sample(c(proposal, current), 
                      size = 1,
                      prob = c(alpha, 1-alpha))


  return(data.frame(proposal, alpha, next_stop))
  
} #end of one_mh_iteration function
```

Using the `one_mh_iteration` function

```{r}
set.seed(4)

one_mh_iteration(sigma = 3, current = 1)
```
  
The code above implements one iteration of the Metropolis-Hastings algorithm but we would like to have multiple iterations.


```{r}
mh_tour <- function(N, sigma){
  
  # Start the chain at location 1
  current <- 13
  
  # Initialize the simulation
  
  lambda <- rep(0, N)
  
  # Simulate N Markov chain stops
  
  for(i in 1:N){
    
    #Simulate one iteration
    sim <- one_mh_iteration(sigma = sigma, 
                            current = current)
    
    #Record next location
    lambda[i] <- sim$next_stop

    #Reset the current location
    current <- sim$next_stop
 

  }
  #Return the chain locations
  return(data.frame(iteration = c(1:N), lambda))
}
```

Using the Metropolis Hastings Algorithm to approximate the posterior. Recall:

$\lambda \sim  \text{Gamma}(1,1)$  
$X = 0$  

We know posterior will be Gamma(1,2)

```{r}
set.seed(4)
mh_simulation_1 <- mh_tour(N = 500, sigma =3)

mh_simulation_1 %>% 
  ggplot(aes(x = iteration, y = lambda)) +
  geom_line()

mh_simulation_1 %>% 
  ggplot(aes(x = lambda)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dgamma, args = list(1,2))

```

##Q2 lambda = 13, n=5000 sigma =3,  plot
```{r}
set.seed(4)
mh_simulation_1 <- mh_tour(N = 5000, sigma =3)

mh_simulation_1 %>% 
  ggplot(aes(x = iteration, y = lambda)) +
  geom_line()

mh_simulation_1 %>% 
  ggplot(aes(x = lambda)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dgamma, args = list(1,2))

```


##Q2 Try sigma = 30




$\lambda \sim  \text{Gamma}(1,1)$  
$X = 0$

We know posterior will be Gamma(1,2)

```{r message=FALSE}
library(tidyverse)
```

```{r}
current <- 13
set.seed(4)
```

MH algorithm makes a proposal

```{r}
proposal <- rnorm(1, mean = current, sd = 30)
proposal
```


```{r}
bayesrules::plot_normal(current, 3)
```


```{r}
proposal_plaus <- dgamma(proposal, 1, 1) * dpois(0, proposal)

current_plaus <- dgamma(current, 1, 1) * dpois(0, current)

alpha <- min(1, proposal_plaus/current_plaus)
alpha
```

Should I stay or should I go?

```{r}
next_stop <- sample(c(proposal, current), 
                    size = 1, 
                    prob = c(alpha, 1-alpha))
next_stop
```



```{r}
one_mh_iteration <- function(sigma, current){
  
  proposal <- rnorm(1, mean = current, sd = sigma)
  
  if (proposal < 0) {
    alpha <- 0
    
  } else {
    
    proposal_plaus <- dgamma(proposal, 1, 1) * dpois(0, proposal)
    
    current_plaus <- dgamma(current, 1, 1) * dpois(0, current)
    
    alpha <- min(1, proposal_plaus/current_plaus)
  }   
  
  next_stop <- sample(c(proposal, current), 
                      size = 1,
                      prob = c(alpha, 1-alpha))


  return(data.frame(proposal, alpha, next_stop))
  
} #end of one_mh_iteration function
```

Using the `one_mh_iteration` function

```{r}
set.seed(4)

one_mh_iteration(sigma = 30, current = 1)
```
  
The code above implements one iteration of the Metropolis-Hastings algorithm but we would like to have multiple iterations.


```{r}
mh_tour <- function(N, sigma){
  
  # Start the chain at location 1
  current <- 13
  
  # Initialize the simulation
  
  lambda <- rep(0, N)
  
  # Simulate N Markov chain stops
  
  for(i in 1:N){
    
    #Simulate one iteration
    sim <- one_mh_iteration(sigma = sigma, 
                            current = current)
    
    #Record next location
    lambda[i] <- sim$next_stop

    #Reset the current location
    current <- sim$next_stop
 

  }
  #Return the chain locations
  return(data.frame(iteration = c(1:N), lambda))
}
```

Using the Metropolis Hastings Algorithm to approximate the posterior. Recall:

$\lambda \sim  \text{Gamma}(1,1)$  
$X = 0$  

We know posterior will be Gamma(1,2)

```{r}
set.seed(4)
mh_simulation_1 <- mh_tour(N = 500, sigma =30)

mh_simulation_1 %>% 
  ggplot(aes(x = iteration, y = lambda)) +
  geom_line()

mh_simulation_1 %>% 
  ggplot(aes(x = lambda)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dgamma, args = list(1,2))

```

##Q2 lambda = 13, n=5000 sigma =30,  plot
```{r}
set.seed(4)
mh_simulation_1 <- mh_tour(N = 5000, sigma =30)

mh_simulation_1 %>% 
  ggplot(aes(x = iteration, y = lambda)) +
  geom_line()

mh_simulation_1 %>% 
  ggplot(aes(x = lambda)) +
  geom_histogram(aes(y = ..density..)) +
  stat_function(fun = dgamma, args = list(1,2))

```

